{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, GRU, Embedding\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# importing data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text = \"\"\"just a rather very intelligent system jarvis was originally tony stark's natural-language user interface computer system.\nnamed after edwin jarvis.Jarvis was designed by tony stark. the butler who worked for howard stark.jarvis is designed by tony stark. over time. he was upgraded into an artificially intelligent system.jarvis is controlled by stark's commands.tasked with running business for stark industries as well as security for tony stark's mansion and stark tower. after creating the mark ii armor.\nstark uploaded jarvis into all of the iron man armors. as well as allowing him to interact with the other avengers. giving them valuable\ninformation during combat. during the ultron offensive. jarvis was destroyed by ultron. although his remaining programming codes unknowingly\ncontinued to thwart ultron's plans of gaining access to nuclear missiles. his remains were found by stark. who uploaded them into a synthetic body made of vibranium and. in conjunction with ultron's personality and an infinity stone. an entirely new being was made: vision. jarvis duties were then taken over by friday.just a rather very intelligent system jarvis was originally tony stark's natural-language user interface computer system.\nnamed after edwin jarvis. the butler who worked for howard stark.jarvis is designed by tony stark. over time. he was upgraded into an artificially intelligent system.jarvis is controlled by stark's commands.tasked with running business for stark industries as well as security for tony stark's mansion and stark tower. after creating the mark ii armor.\nstark uploaded jarvis into all of the iron man armors. as well as allowing him to interact with the other avengers. giving them valuable\ninformation during combat. during the ultron offensive. jarvis was destroyed by ultron. although his remaining programming codes unknowingly\ncontinued to thwart ultron's plans of gaining access to nuclear missiles. his remains were found by stark. who uploaded them into a synthetic body made of vibranium and. in conjunction with ultron's personality and an infinity stone. an entirely new being was made: vision. jarvis duties were then taken over by friday.\"\"\"","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Text Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef text_cleaner(text):\n    # lower case text\n    newString = text.lower()\n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    # remove punctuations\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    long_words=[]\n    # remove short word\n    for i in newString.split():\n        if len(i)>=3:                  \n            long_words.append(i)\n    return (\" \".join(long_words)).strip()\n\n# preprocess the text\ndata_new = text_cleaner(data_text)","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Creating Sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_seq(text):\n    length = 30\n    sequences = list()\n    for i in range(length, len(text)):\n        # select sequence of tokens\n        seq = text[i-length:i+1]\n        # store\n        sequences.append(seq)\n    print('Total Sequences: %d' % len(sequences))\n    return sequences\n\n# create sequences   \nsequences = create_seq(data_new)","execution_count":37,"outputs":[{"output_type":"stream","text":"Total Sequences: 1944\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":">  \nEncoding Sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a character mapping index\nchars = sorted(list(set(data_new)))\nmapping = dict((c, i) for i, c in enumerate(chars))\n\ndef encode_seq(seq):\n    sequences = list()\n    for line in seq:\n        # integer encode line\n        encoded_seq = [mapping[char] for char in line]\n        # store\n        sequences.append(encoded_seq)\n    return sequences\n\n# encode the sequences\nsequences = encode_seq(sequences)\n","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Create Training and Validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# vocabulary size\nvocab = len(mapping)\nsequences = np.array(sequences)\n# create X and y\nX, y = sequences[:,:-1], sequences[:,-1]\n# one hot encode y\ny = to_categorical(y, num_classes=vocab)\n# create train and validation sets\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)","execution_count":39,"outputs":[{"output_type":"stream","text":"Train shape: (1749, 30) Val shape: (195, 30)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"> Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab, 50, input_length=30, trainable=True))\nmodel.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\nmodel.add(Dense(vocab, activation='softmax'))\nprint(model.summary())\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n# fit the model\nmodel.fit(X_tr, y_tr, epochs=60, verbose=2, validation_data=(X_val, y_val))","execution_count":40,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 30, 50)            1200      \n_________________________________________________________________\ngru_2 (GRU)                  (None, 150)               90450     \n_________________________________________________________________\ndense_2 (Dense)              (None, 24)                3624      \n=================================================================\nTotal params: 95,274\nTrainable params: 95,274\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 1749 samples, validate on 195 samples\nEpoch 1/60\n - 3s - loss: 2.9936 - acc: 0.1447 - val_loss: 2.9106 - val_acc: 0.1128\nEpoch 2/60\n - 2s - loss: 2.8168 - acc: 0.1675 - val_loss: 2.8068 - val_acc: 0.1590\nEpoch 3/60\n - 2s - loss: 2.5519 - acc: 0.2842 - val_loss: 2.5562 - val_acc: 0.2410\nEpoch 4/60\n - 2s - loss: 2.2818 - acc: 0.3282 - val_loss: 2.4985 - val_acc: 0.2513\nEpoch 5/60\n - 2s - loss: 2.1304 - acc: 0.3671 - val_loss: 2.3847 - val_acc: 0.3128\nEpoch 6/60\n - 2s - loss: 2.0134 - acc: 0.4002 - val_loss: 2.3269 - val_acc: 0.3231\nEpoch 7/60\n - 2s - loss: 1.9177 - acc: 0.4431 - val_loss: 2.2592 - val_acc: 0.3282\nEpoch 8/60\n - 2s - loss: 1.8152 - acc: 0.4465 - val_loss: 2.2312 - val_acc: 0.3744\nEpoch 9/60\n - 2s - loss: 1.7090 - acc: 0.4786 - val_loss: 2.1093 - val_acc: 0.4205\nEpoch 10/60\n - 2s - loss: 1.6194 - acc: 0.5003 - val_loss: 2.0373 - val_acc: 0.4154\nEpoch 11/60\n - 2s - loss: 1.5188 - acc: 0.5352 - val_loss: 1.9694 - val_acc: 0.4410\nEpoch 12/60\n - 2s - loss: 1.4239 - acc: 0.5529 - val_loss: 1.9127 - val_acc: 0.4564\nEpoch 13/60\n - 2s - loss: 1.3094 - acc: 0.5958 - val_loss: 1.8560 - val_acc: 0.4718\nEpoch 14/60\n - 2s - loss: 1.2054 - acc: 0.6204 - val_loss: 1.7701 - val_acc: 0.5282\nEpoch 15/60\n - 2s - loss: 1.0961 - acc: 0.6501 - val_loss: 1.6502 - val_acc: 0.5641\nEpoch 16/60\n - 2s - loss: 0.9987 - acc: 0.6947 - val_loss: 1.6324 - val_acc: 0.5692\nEpoch 17/60\n - 2s - loss: 0.8914 - acc: 0.7250 - val_loss: 1.5133 - val_acc: 0.6000\nEpoch 18/60\n - 2s - loss: 0.8084 - acc: 0.7667 - val_loss: 1.4238 - val_acc: 0.6308\nEpoch 19/60\n - 2s - loss: 0.7335 - acc: 0.7787 - val_loss: 1.3680 - val_acc: 0.6564\nEpoch 20/60\n - 2s - loss: 0.6644 - acc: 0.8050 - val_loss: 1.3316 - val_acc: 0.6872\nEpoch 21/60\n - 2s - loss: 0.5870 - acc: 0.8359 - val_loss: 1.2467 - val_acc: 0.7231\nEpoch 22/60\n - 2s - loss: 0.5263 - acc: 0.8639 - val_loss: 1.1424 - val_acc: 0.7436\nEpoch 23/60\n - 2s - loss: 0.4717 - acc: 0.8794 - val_loss: 1.1102 - val_acc: 0.7590\nEpoch 24/60\n - 2s - loss: 0.4306 - acc: 0.8891 - val_loss: 1.0307 - val_acc: 0.7795\nEpoch 25/60\n - 2s - loss: 0.3947 - acc: 0.9097 - val_loss: 1.0041 - val_acc: 0.7795\nEpoch 26/60\n - 2s - loss: 0.3594 - acc: 0.9165 - val_loss: 0.9774 - val_acc: 0.7897\nEpoch 27/60\n - 2s - loss: 0.3433 - acc: 0.9137 - val_loss: 0.9680 - val_acc: 0.7949\nEpoch 28/60\n - 2s - loss: 0.3077 - acc: 0.9262 - val_loss: 0.8715 - val_acc: 0.8256\nEpoch 29/60\n - 2s - loss: 0.2926 - acc: 0.9268 - val_loss: 0.8859 - val_acc: 0.8410\nEpoch 30/60\n - 2s - loss: 0.2801 - acc: 0.9348 - val_loss: 0.8445 - val_acc: 0.8513\nEpoch 31/60\n - 2s - loss: 0.2434 - acc: 0.9405 - val_loss: 0.8068 - val_acc: 0.8718\nEpoch 32/60\n - 2s - loss: 0.2138 - acc: 0.9571 - val_loss: 0.7818 - val_acc: 0.8718\nEpoch 33/60\n - 2s - loss: 0.1937 - acc: 0.9605 - val_loss: 0.7985 - val_acc: 0.8872\nEpoch 34/60\n - 2s - loss: 0.1985 - acc: 0.9611 - val_loss: 0.7421 - val_acc: 0.8974\nEpoch 35/60\n - 2s - loss: 0.1654 - acc: 0.9651 - val_loss: 0.7394 - val_acc: 0.8821\nEpoch 36/60\n - 2s - loss: 0.1736 - acc: 0.9617 - val_loss: 0.7594 - val_acc: 0.8923\nEpoch 37/60\n - 2s - loss: 0.1608 - acc: 0.9703 - val_loss: 0.7439 - val_acc: 0.8821\nEpoch 38/60\n - 2s - loss: 0.1466 - acc: 0.9663 - val_loss: 0.7533 - val_acc: 0.8872\nEpoch 39/60\n - 2s - loss: 0.1404 - acc: 0.9686 - val_loss: 0.7203 - val_acc: 0.8974\nEpoch 40/60\n - 2s - loss: 0.1405 - acc: 0.9680 - val_loss: 0.7335 - val_acc: 0.9026\nEpoch 41/60\n - 2s - loss: 0.1266 - acc: 0.9731 - val_loss: 0.7489 - val_acc: 0.9077\nEpoch 42/60\n - 2s - loss: 0.1233 - acc: 0.9743 - val_loss: 0.7456 - val_acc: 0.8974\nEpoch 43/60\n - 2s - loss: 0.1145 - acc: 0.9771 - val_loss: 0.7352 - val_acc: 0.8974\nEpoch 44/60\n - 2s - loss: 0.1240 - acc: 0.9743 - val_loss: 0.7319 - val_acc: 0.8923\nEpoch 45/60\n - 2s - loss: 0.1096 - acc: 0.9766 - val_loss: 0.7447 - val_acc: 0.8974\nEpoch 46/60\n - 2s - loss: 0.1080 - acc: 0.9760 - val_loss: 0.7389 - val_acc: 0.9077\nEpoch 47/60\n - 3s - loss: 0.1052 - acc: 0.9731 - val_loss: 0.7475 - val_acc: 0.8974\nEpoch 48/60\n - 2s - loss: 0.0929 - acc: 0.9800 - val_loss: 0.7204 - val_acc: 0.9077\nEpoch 49/60\n - 2s - loss: 0.0924 - acc: 0.9806 - val_loss: 0.7236 - val_acc: 0.9128\nEpoch 50/60\n - 2s - loss: 0.0844 - acc: 0.9823 - val_loss: 0.7419 - val_acc: 0.8974\nEpoch 51/60\n - 2s - loss: 0.0866 - acc: 0.9800 - val_loss: 0.7390 - val_acc: 0.9026\nEpoch 52/60\n - 2s - loss: 0.0753 - acc: 0.9851 - val_loss: 0.7768 - val_acc: 0.8974\nEpoch 53/60\n - 2s - loss: 0.0845 - acc: 0.9788 - val_loss: 0.7865 - val_acc: 0.8923\nEpoch 54/60\n - 2s - loss: 0.0785 - acc: 0.9811 - val_loss: 0.7705 - val_acc: 0.8872\nEpoch 55/60\n - 2s - loss: 0.0809 - acc: 0.9823 - val_loss: 0.7548 - val_acc: 0.8872\nEpoch 56/60\n - 2s - loss: 0.0804 - acc: 0.9806 - val_loss: 0.7475 - val_acc: 0.9026\nEpoch 57/60\n - 2s - loss: 0.0615 - acc: 0.9891 - val_loss: 0.7448 - val_acc: 0.9026\nEpoch 58/60\n - 2s - loss: 0.0632 - acc: 0.9868 - val_loss: 0.7575 - val_acc: 0.8974\nEpoch 59/60\n - 2s - loss: 0.0586 - acc: 0.9851 - val_loss: 0.7671 - val_acc: 0.8974\nEpoch 60/60\n - 2s - loss: 0.0647 - acc: 0.9846 - val_loss: 0.7525 - val_acc: 0.8974\n","name":"stdout"},{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f8be0158c88>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a sequence of characters with a language model\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\n    in_text = seed_text\n    # generate a fixed number of characters\n    for _ in range(n_chars):\n# encode the characters as integers\n        encoded = [mapping[char] for char in in_text]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # predict character\n        yhat = model.predict_classes(encoded, verbose=0)\n        # reverse map integer to character\n        out_char = ''\n        for char, index in mapping.items():\n            if index == yhat:\n                out_char = char\n                break\n        # append to input\n        in_text += char\n    return in_text","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Required outputs "},{"metadata":{},"cell_type":"markdown","source":"## #1.Question [jarvis was distroyed by?]"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = 'jarvis was distroyed'\nprint(len(inp))\nprint(generate_seq(model,mapping,30,inp.lower(),8))","execution_count":42,"outputs":[{"output_type":"stream","text":"20\njarvis was distroyed ultron \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## #2.Qestion [Jaevis was created ?]"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = 'jarvis designed and created'\nprint(len(inp))\nprint(generate_seq(model,mapping,30,inp.lower(),8))","execution_count":50,"outputs":[{"output_type":"stream","text":"27\njarvis designed and created stark n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## #3.Qestion [Jarvis can objected to stark's]"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = 'jarvis can object to Stark'\nprint(len(inp))\nprint(generate_seq(model,mapping,30,inp.lower(),8))","execution_count":44,"outputs":[{"output_type":"stream","text":"26\njarvis can object to stark uploade\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## #4.Question  [a sophisticated artificial]"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = 'a sophisticated artificial'\nprint(len(inp))\nprint(generate_seq(model,mapping,30,inp.lower(),14))","execution_count":45,"outputs":[{"output_type":"stream","text":"26\na sophisticated artificially intelligent\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Hope you like it .\nplease answer back as soon as possible\ni have tryed to improve my model by the following tecqunics :\n    #Text Cleaning.\n    #Text-to-Number Transformation.\n    #Choosing the Optimal Regression Method\n    #Normalized Corpus"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}